{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Replace 'your_pdf_file.pdf' with the path to your PDF file\n",
    "pdf_file_path = 'KTBS CLass 10th Language Kannada_Part 1.pdf'\n",
    "text_file_path = 'res.txt'\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_file = open(pdf_file_path, 'rb')\n",
    "\n",
    "# Create a PDF object\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "# Initialize an empty string to store the extracted text\n",
    "text = \"\"\n",
    "\n",
    "# Loop through each page and extract text\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    page = pdf_reader.pages[page_num]\n",
    "    text += page.extract_text()\n",
    "\n",
    "# Close the PDF file\n",
    "pdf_file.close()\n",
    "\n",
    "# Write the extracted text to a text file\n",
    "with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "    text_file.write(text)\n",
    "\n",
    "print(f\"Text extracted from '{pdf_file_path}' and saved to '{text_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the web page you want to scrape\n",
    "url = 'https://www.prajavani.net/'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Identify the HTML elements that contain the Kannada text data\n",
    "    # For example, if Kannada headlines are within <h2> tags, you can do this:\n",
    "    kannada_headlines = soup.find_all('h2')\n",
    "\n",
    "    # Iterate through the headlines and extract the text\n",
    "    for headline in kannada_headlines:\n",
    "        kannada_text = headline.text\n",
    "        print(kannada_text)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class KannadaSpider(scrapy.Spider):\n",
    "    name = 'kannada_spider'\n",
    "\n",
    "    start_urls = ['https://www.prajavani.net/']  # Start with the domain's main page\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract Kannada text data from the current page (you need to inspect the website's structure)\n",
    "        kannada_text = response.css('your-css-selector-for-kannada-text').getall()\n",
    "\n",
    "        # Process or save the extracted data as needed (e.g., print it)\n",
    "        for text in kannada_text:\n",
    "            print(text)\n",
    "\n",
    "        # Follow links to other pages if needed\n",
    "        # Replace 'next-page-link-css-selector' and 'callback_method' as appropriate\n",
    "        next_page = response.css('next-page-link-css-selector').attrib['href']\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.callback_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy genspider kannada_spider prajavani.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "visited_pages = set()\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "def parse_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def scrape_website(url):\n",
    "    if url in visited_pages:\n",
    "        return\n",
    "    visited_pages.add(url)\n",
    "    html = fetch_page(url)\n",
    "    if html:\n",
    "        text = parse_page(html)\n",
    "        # Save or process the text as needed\n",
    "        links = [link.get('href') for link in BeautifulSoup(html, 'html.parser').find_all('a') if link.get('href')]\n",
    "\n",
    "        for link in links:\n",
    "            if link.startswith('http'):\n",
    "                scrape_website(link)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website_url = \"https://www.prajavani.net/\"\n",
    "    scrape_website(website_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanahu/VS_Code/GitHub/gpt_from_scratch/venv/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "visited_pages = set()\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "def parse_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def scrape_website(url):\n",
    "    if url in visited_pages:\n",
    "        return\n",
    "    visited_pages.add(url)\n",
    "    html = fetch_page(url)\n",
    "    if html:\n",
    "        text = parse_page(html)\n",
    "        # Save or process the text as needed\n",
    "        with open(\"response.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "            file.write(text + \"\\n\")\n",
    "        links = [link.get('href') for link in BeautifulSoup(html, 'html.parser').find_all('a') if link.get('href')]\n",
    "\n",
    "        for link in links:\n",
    "            if link.startswith('http'):\n",
    "                scrape_website(link)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website_url = \"https://www.prajavani.net\"\n",
    "    scrape_website(website_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
